{"index":{"slug":"index","filePath":"index.md","title":"About Me","links":["posts","projects"],"tags":[],"content":"\n\nHi Iâ€™m Andrew ðŸ‘‹. Iâ€™m a ML/MLOps engineer trying to build fun things.\nHere I write about things Iâ€™m working on, things Iâ€™m learning and things I find interesting.\n\n\nIâ€™m always changing (breaking) this website but here are some things you might be interested in:\n\nPosts, words that Iâ€™ve write\nProjects, things that Iâ€™ve built or am building\n\nWork\nI currently work at Ericsson.\nYou can stalk my LinkedIn if you really want to.\nContact\nYou can reach me at:\nEmail: andrew@goyangi.io\nGitHub: @solanyn"},"posts/deploying-kubeflow":{"slug":"posts/deploying-kubeflow","filePath":"posts/deploying-kubeflow.md","title":"Deploying Kubeflow","links":[],"tags":["machine-learning","kubernetes"],"content":"Deploying the Kubeflow platform\nPreamble\nI recently started contributing to the Kubeflow open-source project and have been following some recent GitHub discussions, watched Working Group (WG) call recordings (they run past midnight in my timezone unfortunately) and community calls. Many contributors come from organizations that deploy Kubeflow internally or offer it as a managed service. One of the more popular requests from the community is to build Helm charts for deploying the Kubeflow platform.\nWhat on earth is Kubeflow?\nThe Kubeflow platform is a collection of machine learning tools designed to be deployed on Kubernetes. Kubeflow is deployed via a community-maintained repository, kubeflow/manifests, which provides a collection of Kustomize manifests for each component. These manifests include Istio configuration, pod security standards, network policies, and token-based authorization. The repository holds a collection of Kustomize manifests upstreamed from each Kubeflow component. It also implements an Istio service mesh which secures traffic between components along with other security measures like pod security standards, network policies, and authorization tokens. It can also be configured to use an external OAuth2 provider. The scope of designing Kubeflow Helm charts is enormous and no easy effort, which makes sense why the Kubeflow organisation chooses not to maintain a central distribution of Kubeflow. Like many open-source projects, contributions often come from developers funded by their organizations or volunteers contributing in their free time. Kubeflow is already extremely complex to maintain to be security compliant for enterprises and has its own challenges designing and coordinating AI/ML tooling on Kubernetes. Vendors tend to provide enterprise support for deploying the platform.\nI recently went through deploying a customised Kubeflow. I operate a bare-metal Kubernetes cluster using GitOps powered by Flux. A Flux controller in my cluster reads the latest changes in the repository and synchronises changes to my cluster. I usually reach for Helm charts since I find that they are the simplest to configure but occasionally deploy operators and/or Custom Resource Definitions (CRDs) from manifests if this is not an option. I recently moved away from Ingress NGINX Controller to the new Gateway API using the Cilium Gateway implementation. This posed challenges because Istioâ€™s Gateway implementation and the Gateway APIâ€™s evolving spec introduced compatibility and configuration issues. This should be resolved with the next Kubeflow platform release.\nDeploying the Kubeflow platform has a few external dependencies:\n\nS3-compatible storage\nMySQL/MySQL-compatible database\nIstio service mesh\nCert Manager for certificates\n\nI already deploy Cert Manager for certificates for my split-brain DNS setup to publicly expose services via Cloudflare DNS and Cloudflare tunnels and internal (only on my network) using External DNS and the external-dns-unifi-webhook and a MinIO deployment with an NFS backend to my NAS. I settled on MariaDB Operator for its convenient CRDs to create databases, users and grants and scheduled S3 backup and restore procedures.\nKubeflow and Fluxtomizations\nI largely used the deployKF repository as inspiration which is a more configurable setup using gomplate, ArgoCD and raw bash scripts to configure settings. Itâ€™s also a couple of revisions old (my guess is the maintainer started vendoring the distribution for later Kubeflow versions). It uses the same mechanism Helm values are templated into charts to create customised deployments AND kustomizations. I ended up opting for configurable options through Kubernetes Secret and ConfigMap objects. One of the biggest challenges was identifying all the points where MySQL and MinIO credentials were needed and using Kustomize patches to use a centralised secret. I should mention that secrets can be populated manually, using GitOps with sops encryption (natively supported in Flux) or use External Secrets Operator to populate secrets from an external secret store. I personally use 1Password to populate my secrets which allows me to conveniently autofill credentials.\nRoughly speaking I went through the following steps:\n\nWrite a GitHub Actions workflow to commit the latest non-release candidate tag of the manifests to my repository\nAdd components to generate namespaces with the appropriate labels e.g., for Istio injection\nConfigure secrets and OAuth2 settings for dex and oauth2-proxy using ExternalSecret and ConfigMap resources\nAdd Kustomize patches to patch dex and oauth2-proxy deployments to mount the the configuration from secrets and/or configmaps\nAdd an ExternalSecret manifest with configurations for ALL Kubeflow components\nCreate Database objects for Kubeflow components that use MySQL/MySQL-compatible databases\nAdd Kustomize patches to patch Kubeflow components (mainly Katib, Pipelines and Model Registry) to connect to my MariaDB cluster and the appropriate tables\nAdd Kustomize patches to patch Kubeflow Pipelines to connect to my MinIO deployment\nAdd any additional Istio RequestAuthentication manifests to enable authorization from GitHub and Pocket ID\n\nConclusion\nWhile future changes to the manifests could require rework, using Kustomize patches makes changes relatively easy to reconfigure. Being a contributor helps here, I would be aware of any breaking changes that could be introduced. Looking forward to the ongoing improvements like Gateway API support and Istio Ambient mode and how things will break in my cluster! Thanks for reading."},"posts/k8s-at-home":{"slug":"posts/k8s-at-home","filePath":"posts/k8s-at-home.md","title":"Kubernetes at Home","links":[],"tags":["kubernetes"],"content":"Running Kubernetes at Home\nIâ€™ve been managing a bare-metal Kubernetes cluster at home for a while now to learn about Kubernetes and its ecosystem. You can check it out here: solanyn/home-ops\nThe Cluster\nMy Kubernetes cluster is deployed with Talos and is managed by Flux via GitOps. I handle backups of the cluster to my NAS via NFS and S3 and offsite to Cloudflare R2.\nCore Components\n\nFlux: GitOps operator that reconciles cluster state from Git\nRenovate: Automatically updates dependencies and creates PRs via GitHub Actions\ncert-manager: Manages SSL certificates across services\nexternal-dns: Syncs DNS records for ingresses, services and Gateway API HTTP routes\nexternal-secrets: Integrates Kubernetes Secrets with 1Password\ncilium: High-performance networking for Kubernetes\nrook: Provides persistent storage for the cluster\nvolsync: Provides volume replication and dissaster recovery\n\nDNS with Split Horizon\nI run two instances of external-dns:\n\nOne syncs private DNS records to my UniFi router using a webhook provider\nThe other pushes public records to Cloudflare\n\nIngresses/HTTPRoutes are tagged with internal or external classes to control which DNS provider is used.\nFinal Comments\nThis setup has been a great learning experience and has allowed me to explore the Kubernetes ecosystem in depth. I plan on building and expanding my cluster with compute, deploying more complex platforms and services (looking at you Kubeflow) and building AI/ML workloads on Kubernetes!"}}